This report describes the data cleaning and preprocessing steps performed on the dataset cs_bisnode_panel.csv to prepare it for firm-level analysis and machine learning modeling. The goal was to construct a structured, reliable panel dataset that enables the study of firm growth and performance over time.

The process began with loading the dataset and removing several variables with a high proportion of missing values, including COGS, finished_prod, net_dom_sales, net_exp_sales, and wages. Observations corresponding to the year 2016 were also excluded from the analysis. To ensure that the panel was balanced, the dataset was expanded to include all possible combinations of year and comp_id using the complete() function, filling in missing rows where necessary.

A key variable, status_alive, was created to identify whether a firm was operational in a given year. This was defined as having strictly positive and non-missing sales. The dataset was further filtered to include only firms that were active in either 2012 or 2014, the two focal years for analysis. Sales values were cleaned by replacing any negative sales figures with a minimum value of 1 to avoid issues during log transformation.

Firm age was computed based on the difference between the year and the founding year, with any negative values set to zero. A binary indicator variable called new was generated to identify newly established firms (those aged 0 or 1 year) that also had an incomplete balance sheet year. Outliers were removed by filtering out firms with sales above 10 million or below 1,000.

Next, the industry classification (ind2) was recoded into broader categories under a new variable called ind2_cat. This involved grouping various codes into categories such as 20, 30, 40, and 60 based on business logic. Firms with unclassified or missing industry codes were excluded from further analysis. Additional features were engineered, including age^2, a binary indicator for foreign management, gender composition of the management team, and regional location of the firm.

Negative asset values were corrected by replacing them with zeros, and firms with any negative assets were flagged using a new variable called flag_asset_problem. The balance sheet total (total_assets_bs) was computed as the sum of intangible, current, and fixed assets. A list of variables that should not be negative—such as sales, curr_assets, fixed_assets, inventories, etc.—was used to filter out invalid observations, and these variables were subsequently transformed using the natural log (log1p()), creating new variables prefixed with ln_. After transformation, the original versions of these variables were removed, except for sales and total_assets_bs.

Two subsets were created for 2012 and 2014, which were then merged based on the firm identifier (comp_id). Financial variables in both years were normalized by the firm's total assets in 2012 to ensure comparability across firms of different sizes. Differences and growth rates were then calculated between the two years. For growth, the log difference between 2012 and 2014 values was computed, while for net values (such as profit or equity), the raw difference was used.

To further clean the dataset, several variables and columns unrelated to the analysis—such as exit year, various balance sheet flags, and unused categorical features—were dropped. Variables with excessive missing data were also removed, and any remaining rows with NA values were dropped entirely.

Growth rates and changes in key financial indicators were then analyzed. Dummy variables were created to capture positive growth, extreme increases (over 100%), and severe declines (greater than 50% drop) for each variable. These dummy variables were encoded as categorical factors with "yes" and "no" values. Filters were applied to remove extreme values in the change of profit, tax, and equity indicators to avoid outliers that could skew modeling.

Finally, the dataset was refined to exclude firms with industry code 99, and an additional variable industry_type was created to distinguish between manufacturing and service firms. The firm’s growth rate in sales was also calculated using the exponential of the log growth in sales. The cleaned and fully processed dataset, now ready for modeling, was exported as data_cleaned.csv.

This data cleaning pipeline ensures that the final dataset includes only valid, reliable observations for firms that were active in both 2012 and 2014. It captures a wide array of engineered features, growth indicators, and dummy variables, setting a strong foundation for downstream analysis and machine learning applications.
